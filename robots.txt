# robots.txt for Yandux.Biz
# Last updated: June 11, 2024

# --- Default Crawling Rules ---
# We allow most bots by default, including major search engines like Google, Bing, and Yandex.
User-agent: *
Allow: /
# Disallow crawling of dynamic search results and script pages for all bots.
Disallow: /search?keyword=
Disallow: /download-proxy.php


# --- Archival Bots ---
# We specifically welcome bots that archive the web for historical and research purposes.
# By not having a "Disallow" rule, bots like Internet Archive (archive.org_bot) 
# and Common Crawl (CCBot) are permitted to crawl based on the wildcard rule above.


# --- Blocked Bots ---
# The following bots are disallowed from crawling the site.
# This includes some SEO tools, aggressive crawlers, AI data scrapers, and other unwanted bots.

# Block specific SEO/aggressive crawlers.
User-agent: MJ12bot
Disallow: /
User-agent: AhrefsBot
Disallow: /
User-agent: SemrushBot
Disallow: /
User-agent: BLEXBot
Disallow: /
User-agent: DotBot
Disallow: /
User-agent: SeznamBot
Disallow: /
User-agent: petalbot
Disallow: /

# Block common AI and data collection bots.
User-agent: Google-Extended
Disallow: /
User-agent: GPTBot
Disallow: /
User-agent: ChatGPT-User
Disallow: /
User-agent: ClaudeBot
Disallow: /
User-agent: PerplexityBot
Disallow: /
User-agent: Anthropic-AI
Disallow: /

# Block email harvesters and other malicious bots.
User-agent: EmailCollector
Disallow: /
User-agent: WebEMAilExtrac
Disallow: /
User-agent: Exabot
Disallow: /
User-agent: ExtractorPro
Disallow: /
User-agent: CherryPicker
Disallow: /
User-agent: EirGrabber
Disallow: /


# --- Sitemap Location ---
# Ensure this path is correct and the sitemap is accessible.
Sitemap: https://Yandux.Biz/sitemap.xml
